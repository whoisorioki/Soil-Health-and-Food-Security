{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84fce8e",
   "metadata": {},
   "source": [
    "# üîç Data Exploration & Discovery: Uncovering Natural Stories\n",
    "\n",
    "**Objective**: Let the data speak first. Before crafting narratives, we'll explore our master datasets to discover what stories emerge naturally from patterns, distributions, and relationships.\n",
    "\n",
    "**Approach**: Data-driven storytelling - starting with comprehensive visualization and pattern discovery to build authentic narratives grounded in genuine data insights.\n",
    "\n",
    "---\n",
    "\n",
    "**Master Datasets**:\n",
    "- `risk_assessment_complete.csv` - Sub-regional risk analysis (4,147 regions)\n",
    "- `country_summary.csv` - Country-level aggregations  \n",
    "- Atlas Explorer data fusion results\n",
    "\n",
    "Let's discover what stories our data wants to tell us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6ef1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.10)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Adrian/Desktop/Hackathon/Soil Health and Food Security/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"üìö Libraries loaded successfully!\")\n",
    "print(\"üé® Visualization settings configured\")\n",
    "print(\"üîç Ready for data exploration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ec0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Examine Master Data\n",
    "print(\"üìÇ LOADING MASTER DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the primary datasets\n",
    "try:\n",
    "    # Main risk assessment data\n",
    "    risk_data = pd.read_csv('../data/processed/risk_assessment_complete.csv')\n",
    "    print(f\"‚úÖ Risk assessment data loaded: {risk_data.shape}\")\n",
    "    \n",
    "    # Country-level summary\n",
    "    country_data = pd.read_csv('../submission/data/country_summary.csv')\n",
    "    print(f\"‚úÖ Country summary data loaded: {country_data.shape}\")\n",
    "    \n",
    "    # Try to load other available datasets\n",
    "    try:\n",
    "        compound_risk = pd.read_csv('../data/processed/compound_risk_assessment.csv')\n",
    "        print(f\"‚úÖ Compound risk data loaded: {compound_risk.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ÑπÔ∏è Compound risk data not found\")\n",
    "        compound_risk = None\n",
    "        \n",
    "    try:\n",
    "        master_atlas = pd.read_csv('../data/processed/master_atlas_data_cleaned.csv')\n",
    "        print(f\"‚úÖ Master atlas data loaded: {master_atlas.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ÑπÔ∏è Master atlas data not found\")\n",
    "        master_atlas = None\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Please ensure the data files are in the correct location\")\n",
    "\n",
    "print(\"\\nüîç INITIAL DATA EXAMINATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Examine the main risk data structure\n",
    "print(\"\\nüìä RISK DATA OVERVIEW:\")\n",
    "print(f\"Shape: {risk_data.shape}\")\n",
    "print(f\"Columns: {len(risk_data.columns)}\")\n",
    "print(\"\\nColumn names:\")\n",
    "for i, col in enumerate(risk_data.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Basic info about data types\n",
    "print(f\"\\nüìà DATA TYPES:\")\n",
    "print(risk_data.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nüåç GEOGRAPHIC COVERAGE:\")\n",
    "if 'country' in risk_data.columns:\n",
    "    print(f\"Countries: {risk_data['country'].nunique()}\")\n",
    "    print(f\"Sample countries: {sorted(risk_data['country'].unique())[:5]}...\")\n",
    "if 'region' in risk_data.columns:\n",
    "    print(f\"Regions: {risk_data['region'].nunique()}\")\n",
    "if 'sub_region' in risk_data.columns:\n",
    "    print(f\"Sub-regions: {risk_data['sub_region'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚ú® Data loaded and ready for exploration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Cleaning\n",
    "print(\"üßπ DATA PREPROCESSING & QUALITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n‚ö†Ô∏è MISSING VALUES ANALYSIS:\")\n",
    "missing_data = risk_data.isnull().sum()\n",
    "missing_percentage = (missing_data / len(risk_data)) * 100\n",
    "\n",
    "# Create missing data summary\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Show only columns with missing data\n",
    "missing_cols = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    for col, row in missing_cols.head(10).iterrows():\n",
    "        print(f\"  {col}: {row['Missing_Count']:,} ({row['Missing_Percentage']:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = risk_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = risk_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìä COLUMN TYPES:\")\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Look for potential key risk indicators\n",
    "risk_keywords = ['risk', 'score', 'vulnerability', 'hazard', 'exposure']\n",
    "potential_risk_cols = [col for col in risk_data.columns \n",
    "                      if any(keyword in col.lower() for keyword in risk_keywords)]\n",
    "\n",
    "print(f\"\\nüéØ POTENTIAL RISK INDICATORS ({len(potential_risk_cols)} found):\")\n",
    "for col in potential_risk_cols:\n",
    "    if col in numeric_cols:\n",
    "        values = risk_data[col].dropna()\n",
    "        print(f\"  {col}: {values.min():.3f} - {values.max():.3f} (mean: {values.mean():.3f})\")\n",
    "\n",
    "# Identify geographic columns\n",
    "geo_keywords = ['country', 'region', 'admin', 'lat', 'lon', 'coord']\n",
    "geo_cols = [col for col in risk_data.columns \n",
    "           if any(keyword in col.lower() for keyword in geo_keywords)]\n",
    "\n",
    "print(f\"\\nüåç GEOGRAPHIC COLUMNS ({len(geo_cols)} found):\")\n",
    "for col in geo_cols:\n",
    "    print(f\"  {col}: {risk_data[col].dtype}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585317de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistical Summary\n",
    "print(\"üìà DESCRIPTIVE STATISTICS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate comprehensive statistics for numeric columns\n",
    "numeric_stats = risk_data[numeric_cols].describe()\n",
    "print(\"\\nüìä NUMERIC VARIABLES SUMMARY:\")\n",
    "print(numeric_stats.round(3))\n",
    "\n",
    "# Focus on key risk indicators if they exist\n",
    "if potential_risk_cols:\n",
    "    print(f\"\\nüéØ KEY RISK INDICATORS DETAILED STATS:\")\n",
    "    risk_numeric = [col for col in potential_risk_cols if col in numeric_cols]\n",
    "    \n",
    "    for col in risk_numeric[:5]:  # Show top 5 risk columns\n",
    "        values = risk_data[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Count: {len(values):,}\")\n",
    "            print(f\"  Mean: {values.mean():.4f}\")\n",
    "            print(f\"  Std: {values.std():.4f}\")\n",
    "            print(f\"  Min: {values.min():.4f}\")\n",
    "            print(f\"  25%: {values.quantile(0.25):.4f}\")\n",
    "            print(f\"  50%: {values.quantile(0.50):.4f}\")\n",
    "            print(f\"  75%: {values.quantile(0.75):.4f}\")\n",
    "            print(f\"  Max: {values.max():.4f}\")\n",
    "\n",
    "# Categorical variables summary\n",
    "print(f\"\\nüìã CATEGORICAL VARIABLES SUMMARY:\")\n",
    "for col in categorical_cols[:5]:  # Show top 5 categorical columns\n",
    "    unique_count = risk_data[col].nunique()\n",
    "    most_common = risk_data[col].value_counts().head(3)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {unique_count}\")\n",
    "    print(f\"  Most common:\")\n",
    "    for value, count in most_common.items():\n",
    "        percentage = (count / len(risk_data)) * 100\n",
    "        print(f\"    {value}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for potential data quality issues\n",
    "print(f\"\\nüîç DATA QUALITY CHECKS:\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = risk_data.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates:,}\")\n",
    "\n",
    "# Check for negative values in risk scores (if they should be positive)\n",
    "for col in risk_numeric:\n",
    "    if 'score' in col.lower() or 'risk' in col.lower():\n",
    "        negative_count = (risk_data[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"Negative values in {col}: {negative_count:,}\")\n",
    "\n",
    "# Check for extremely high outliers\n",
    "for col in risk_numeric:\n",
    "    if col in numeric_cols:\n",
    "        values = risk_data[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            q99 = values.quantile(0.99)\n",
    "            outliers = (values > q99 * 3).sum()  # Values > 3x the 99th percentile\n",
    "            if outliers > 0:\n",
    "                print(f\"Extreme outliers in {col}: {outliers:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Statistical analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ebaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Visualizations\n",
    "print(\"üìä UNIVARIATE ANALYSIS: Understanding Individual Variables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comprehensive univariate visualizations\n",
    "def create_distribution_plots():\n",
    "    \"\"\"Create distribution plots for key numeric variables\"\"\"\n",
    "    \n",
    "    # Select top risk indicators for detailed analysis\n",
    "    key_risk_cols = [col for col in potential_risk_cols if col in numeric_cols][:4]\n",
    "    \n",
    "    if len(key_risk_cols) == 0:\n",
    "        print(\"‚ö†Ô∏è No numeric risk columns found, using first 4 numeric columns\")\n",
    "        key_risk_cols = numeric_cols[:4]\n",
    "    \n",
    "    # Create subplots for distributions\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[col.replace('_', ' ').title() for col in key_risk_cols],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for i, col in enumerate(key_risk_cols):\n",
    "        row = (i // 2) + 1\n",
    "        col_pos = (i % 2) + 1\n",
    "        \n",
    "        # Get clean data\n",
    "        data = risk_data[col].dropna()\n",
    "        \n",
    "        # Create histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=data,\n",
    "                name=col,\n",
    "                marker_color=colors[i],\n",
    "                opacity=0.7,\n",
    "                nbinsx=30\n",
    "            ),\n",
    "            row=row, col=col_pos\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Distribution of Key Risk Variables\",\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display distribution plots\n",
    "dist_fig = create_distribution_plots()\n",
    "dist_fig.show()\n",
    "\n",
    "# Box plots for identifying outliers\n",
    "def create_box_plots():\n",
    "    \"\"\"Create box plots to identify outliers and distribution shape\"\"\"\n",
    "    \n",
    "    key_cols = [col for col in potential_risk_cols if col in numeric_cols][:6]\n",
    "    if len(key_cols) == 0:\n",
    "        key_cols = numeric_cols[:6]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i, col in enumerate(key_cols):\n",
    "        data = risk_data[col].dropna()\n",
    "        fig.add_trace(go.Box(\n",
    "            y=data,\n",
    "            name=col.replace('_', ' ').title(),\n",
    "            boxpoints='outliers'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Box Plots: Distribution Shape and Outliers\",\n",
    "        yaxis_title=\"Values\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "box_fig = create_box_plots()\n",
    "box_fig.show()\n",
    "\n",
    "# Geographic distribution if country data exists\n",
    "if 'country' in risk_data.columns:\n",
    "    print(f\"\\nüåç GEOGRAPHIC DISTRIBUTION:\")\n",
    "    \n",
    "    country_counts = risk_data['country'].value_counts().head(15)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=country_counts.values,\n",
    "        y=country_counts.index,\n",
    "        orientation='h',\n",
    "        title=\"Number of Sub-regions by Country (Top 15)\",\n",
    "        labels={'x': 'Number of Sub-regions', 'y': 'Country'}\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Countries with most sub-regions:\")\n",
    "    for country, count in country_counts.head(5).items():\n",
    "        percentage = (count / len(risk_data)) * 100\n",
    "        print(f\"  {country}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Univariate analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis and Correlations\n",
    "print(\"üîó BIVARIATE ANALYSIS: Discovering Relationships\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate correlation matrix for numeric variables\n",
    "numeric_data = risk_data[numeric_cols].select_dtypes(include=[np.number])\n",
    "\n",
    "if len(numeric_data.columns) > 1:\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = numeric_data.corr()\n",
    "    \n",
    "    # Create interactive correlation heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix.values,\n",
    "        x=correlation_matrix.columns,\n",
    "        y=correlation_matrix.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=correlation_matrix.round(3).values,\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 10},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Correlation Matrix: Numeric Variables\",\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Find strongest correlations\n",
    "    print(\"\\nüî• STRONGEST CORRELATIONS:\")\n",
    "    \n",
    "    # Get upper triangle of correlation matrix\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "    correlations = correlation_matrix.where(mask).stack().sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Top 10 strongest correlations:\")\n",
    "    for i, (pair, corr) in enumerate(correlations.head(10).items()):\n",
    "        var1, var2 = pair\n",
    "        print(f\"  {i+1:2d}. {var1} ‚Üî {var2}: {corr:.3f}\")\n",
    "\n",
    "# Risk factors relationship analysis\n",
    "if potential_risk_cols:\n",
    "    print(f\"\\nüéØ RISK FACTORS RELATIONSHIPS:\")\n",
    "    \n",
    "    # Select key risk variables for detailed analysis\n",
    "    key_risk_vars = [col for col in potential_risk_cols if col in numeric_cols][:4]\n",
    "    \n",
    "    if len(key_risk_vars) >= 2:\n",
    "        # Create scatter plot matrix for risk variables\n",
    "        fig = make_subplots(\n",
    "            rows=len(key_risk_vars), cols=len(key_risk_vars),\n",
    "            subplot_titles=[col.replace('_', ' ').title() for col in key_risk_vars]\n",
    "        )\n",
    "        \n",
    "        for i, var1 in enumerate(key_risk_vars):\n",
    "            for j, var2 in enumerate(key_risk_vars):\n",
    "                if i != j:\n",
    "                    # Scatter plot\n",
    "                    clean_data = risk_data[[var1, var2]].dropna()\n",
    "                    \n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=clean_data[var2],\n",
    "                            y=clean_data[var1],\n",
    "                            mode='markers',\n",
    "                            marker=dict(size=3, opacity=0.6),\n",
    "                            name=f\"{var1} vs {var2}\",\n",
    "                            showlegend=False\n",
    "                        ),\n",
    "                        row=i+1, col=j+1\n",
    "                    )\n",
    "                elif i == j:\n",
    "                    # Diagonal: histogram\n",
    "                    data = risk_data[var1].dropna()\n",
    "                    fig.add_trace(\n",
    "                        go.Histogram(\n",
    "                            x=data,\n",
    "                            name=var1,\n",
    "                            showlegend=False,\n",
    "                            opacity=0.7\n",
    "                        ),\n",
    "                        row=i+1, col=j+1\n",
    "                    )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Risk Variables Relationship Matrix\",\n",
    "            height=600,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "# Geographic patterns analysis\n",
    "if 'country' in risk_data.columns and potential_risk_cols:\n",
    "    print(f\"\\nüåç GEOGRAPHIC PATTERNS:\")\n",
    "    \n",
    "    # Select a key risk variable for geographic analysis\n",
    "    main_risk_var = potential_risk_cols[0] if potential_risk_cols[0] in numeric_cols else numeric_cols[0]\n",
    "    \n",
    "    # Country-level risk distribution\n",
    "    country_risk = risk_data.groupby('country')[main_risk_var].agg(['mean', 'std', 'count']).round(3)\n",
    "    country_risk = country_risk.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 countries by average {main_risk_var}:\")\n",
    "    for country, row in country_risk.head(10).iterrows():\n",
    "        print(f\"  {country}: {row['mean']:.3f} (¬±{row['std']:.3f}, n={row['count']})\")\n",
    "    \n",
    "    # Create geographic risk visualization\n",
    "    fig = px.bar(\n",
    "        x=country_risk.head(15).index,\n",
    "        y=country_risk.head(15)['mean'],\n",
    "        error_y=country_risk.head(15)['std'],\n",
    "        title=f\"Average {main_risk_var.replace('_', ' ').title()} by Country\",\n",
    "        labels={'x': 'Country', 'y': f'Average {main_risk_var}'}\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n‚úÖ Bivariate analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Insights and Pattern Discovery\n",
    "print(\"üí° DATA INSIGHTS & PATTERN DISCOVERY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üéØ SYNTHESIZING FINDINGS TO IDENTIFY NATURAL STORIES\")\n",
    "\n",
    "# Function to identify potential stories in the data\n",
    "def discover_data_stories():\n",
    "    \"\"\"Analyze patterns to identify compelling data stories\"\"\"\n",
    "    \n",
    "    stories = []\n",
    "    insights = []\n",
    "    \n",
    "    # 1. Geographic Inequality Story\n",
    "    if 'country' in risk_data.columns and potential_risk_cols:\n",
    "        main_risk = potential_risk_cols[0] if potential_risk_cols[0] in numeric_cols else numeric_cols[0]\n",
    "        country_stats = risk_data.groupby('country')[main_risk].agg(['mean', 'count']).round(3)\n",
    "        \n",
    "        # Find countries with extreme values\n",
    "        highest_risk = country_stats.loc[country_stats['mean'].idxmax()]\n",
    "        lowest_risk = country_stats.loc[country_stats['mean'].idxmin()]\n",
    "        \n",
    "        risk_range = country_stats['mean'].max() - country_stats['mean'].min()\n",
    "        risk_ratio = country_stats['mean'].max() / country_stats['mean'].min() if country_stats['mean'].min() > 0 else float('inf')\n",
    "        \n",
    "        stories.append({\n",
    "            'title': 'Geographic Inequality Story',\n",
    "            'description': f\"Extreme variation in {main_risk} across countries\",\n",
    "            'key_facts': [\n",
    "                f\"Highest risk: {country_stats['mean'].idxmax()} ({highest_risk['mean']:.3f})\",\n",
    "                f\"Lowest risk: {country_stats['mean'].idxmin()} ({lowest_risk['mean']:.3f})\",\n",
    "                f\"Risk range: {risk_range:.3f}\",\n",
    "                f\"Risk ratio: {risk_ratio:.1f}x difference\"\n",
    "            ],\n",
    "            'story_potential': 'High' if risk_ratio > 2 else 'Medium'\n",
    "        })\n",
    "    \n",
    "    # 2. Scale vs Severity Story\n",
    "    if 'country' in risk_data.columns and len(potential_risk_cols) >= 2:\n",
    "        # Look for population/economic exposure data\n",
    "        exposure_cols = [col for col in numeric_cols if any(term in col.lower() \n",
    "                        for term in ['population', 'value', 'economic', 'gdp', 'vop'])]\n",
    "        \n",
    "        if exposure_cols:\n",
    "            exposure_col = exposure_cols[0]\n",
    "            risk_col = potential_risk_cols[0] if potential_risk_cols[0] in numeric_cols else numeric_cols[0]\n",
    "            \n",
    "            country_analysis = risk_data.groupby('country').agg({\n",
    "                risk_col: 'mean',\n",
    "                exposure_col: 'sum'\n",
    "            }).round(3)\n",
    "            \n",
    "            # Find high-risk vs high-exposure countries\n",
    "            high_risk_country = country_analysis[risk_col].idxmax()\n",
    "            high_exposure_country = country_analysis[exposure_col].idxmax()\n",
    "            \n",
    "            stories.append({\n",
    "                'title': 'Scale vs Severity Story',\n",
    "                'description': f\"Different types of vulnerability: severity vs exposure\",\n",
    "                'key_facts': [\n",
    "                    f\"Highest risk severity: {high_risk_country} ({country_analysis.loc[high_risk_country, risk_col]:.3f})\",\n",
    "                    f\"Highest exposure: {high_exposure_country} ({country_analysis.loc[high_exposure_country, exposure_col]:,.0f})\",\n",
    "                    f\"Same country?: {'Yes' if high_risk_country == high_exposure_country else 'No'}\"\n",
    "                ],\n",
    "                'story_potential': 'High' if high_risk_country != high_exposure_country else 'Medium'\n",
    "            })\n",
    "    \n",
    "    # 3. Distribution Pattern Story\n",
    "    if potential_risk_cols:\n",
    "        main_risk = potential_risk_cols[0] if potential_risk_cols[0] in numeric_cols else numeric_cols[0]\n",
    "        risk_values = risk_data[main_risk].dropna()\n",
    "        \n",
    "        # Analyze distribution characteristics\n",
    "        skewness = risk_values.skew()\n",
    "        q25, q50, q75 = risk_values.quantile([0.25, 0.5, 0.75])\n",
    "        extreme_high = (risk_values > q75 + 1.5 * (q75 - q25)).sum()\n",
    "        extreme_low = (risk_values < q25 - 1.5 * (q75 - q25)).sum()\n",
    "        \n",
    "        stories.append({\n",
    "            'title': 'Risk Distribution Pattern',\n",
    "            'description': f\"How {main_risk} is distributed across regions\",\n",
    "            'key_facts': [\n",
    "                f\"Distribution skew: {skewness:.2f} ({'Right-skewed' if skewness > 0.5 else 'Left-skewed' if skewness < -0.5 else 'Roughly normal'})\",\n",
    "                f\"Median: {q50:.3f}\",\n",
    "                f\"High outliers: {extreme_high:,} regions\",\n",
    "                f\"Low outliers: {extreme_low:,} regions\"\n",
    "            ],\n",
    "            'story_potential': 'High' if abs(skewness) > 1 or extreme_high > len(risk_values) * 0.05 else 'Medium'\n",
    "        })\n",
    "    \n",
    "    # 4. Correlation Insights Story\n",
    "    if len(numeric_cols) > 2:\n",
    "        corr_matrix = risk_data[numeric_cols].corr()\n",
    "        \n",
    "        # Find strongest correlations (excluding self-correlations)\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "        strong_corrs = corr_matrix.where(mask).stack()\n",
    "        strongest_corr = strong_corrs.loc[strong_corrs.abs().idxmax()]\n",
    "        strongest_pair = strong_corrs.abs().idxmax()\n",
    "        \n",
    "        # Count very strong correlations\n",
    "        very_strong = (strong_corrs.abs() > 0.8).sum()\n",
    "        strong = (strong_corrs.abs() > 0.6).sum()\n",
    "        \n",
    "        stories.append({\n",
    "            'title': 'Variable Relationships Story',\n",
    "            'description': \"How different risk factors relate to each other\",\n",
    "            'key_facts': [\n",
    "                f\"Strongest correlation: {strongest_pair[0]} ‚Üî {strongest_pair[1]} ({strongest_corr:.3f})\",\n",
    "                f\"Very strong correlations (>0.8): {very_strong}\",\n",
    "                f\"Strong correlations (>0.6): {strong}\",\n",
    "                f\"Relationship type: {'Positive' if strongest_corr > 0 else 'Negative'}\"\n",
    "            ],\n",
    "            'story_potential': 'High' if abs(strongest_corr) > 0.7 else 'Medium'\n",
    "        })\n",
    "    \n",
    "    return stories\n",
    "\n",
    "# Discover and display stories\n",
    "data_stories = discover_data_stories()\n",
    "\n",
    "print(f\"\\nüé¨ DISCOVERED {len(data_stories)} POTENTIAL DATA STORIES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, story in enumerate(data_stories, 1):\n",
    "    print(f\"\\nüìñ STORY {i}: {story['title']}\")\n",
    "    print(f\"Description: {story['description']}\")\n",
    "    print(f\"Story Potential: {story['story_potential']}\")\n",
    "    print(\"Key Facts:\")\n",
    "    for fact in story['key_facts']:\n",
    "        print(f\"  ‚Ä¢ {fact}\")\n",
    "\n",
    "# Recommend the most compelling stories\n",
    "high_potential_stories = [s for s in data_stories if s['story_potential'] == 'High']\n",
    "\n",
    "print(f\"\\n‚≠ê RECOMMENDED STORIES FOR NARRATIVE DEVELOPMENT:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if high_potential_stories:\n",
    "    for i, story in enumerate(high_potential_stories, 1):\n",
    "        print(f\"\\nüåü PRIORITY {i}: {story['title']}\")\n",
    "        print(f\"Why compelling: {story['description']}\")\n",
    "        \n",
    "        # Suggest narrative angle\n",
    "        if 'inequality' in story['title'].lower():\n",
    "            print(\"üìù Narrative angle: 'Tale of Two Extremes' - contrast countries with highest vs lowest risk\")\n",
    "        elif 'scale vs severity' in story['title'].lower():\n",
    "            print(\"üìù Narrative angle: 'Different Faces of Vulnerability' - severity vs exposure comparison\")\n",
    "        elif 'distribution' in story['title'].lower():\n",
    "            print(\"üìù Narrative angle: 'The Hidden Majority vs The Vulnerable Few' - outlier analysis\")\n",
    "        elif 'relationship' in story['title'].lower():\n",
    "            print(\"üìù Narrative angle: 'Connected Crises' - how different factors compound risks\")\n",
    "else:\n",
    "    print(\"All stories have medium potential - consider combining multiple stories for stronger narrative\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pattern discovery complete! {len(high_potential_stories)} high-potential stories identified.\")\n",
    "print(\"üéØ Ready to build authentic, data-driven narratives!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
