#!/usr/bin/env python3
"""
Step 2: Geospatial Processing & Risk Calculation
==============================================

This script integrates SoilGrids environmental data with Atlas socio-economic data
to create a comprehensive compound risk assessment for Sub-Saharan Africa.

Following the PRD narrative structure:
1. Load Atlas master data (from Step 1)
2. Process SoilGrids raster data via zonal statistics
3. Calculate environmental vulnerability scores
4. Compute final compound risk scores
5. Export visualization-ready datasets

Risk Formula: Risk = Hazard Ã— Combined_Vulnerability
Where: Combined_Vulnerability = f(Social_Poverty, Environmental_Soil_Degradation)
"""

import pandas as pd
import geopandas as gpd
from rasterstats import zonal_stats
from pathlib import Path
import rasterio
from rasterio.warp import calculate_default_transform, reproject, Resampling
import tempfile
import numpy as np
import sys
import warnings
warnings.filterwarnings('ignore')

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent))
from config import Config

def setup_paths():
    """Configure all data paths and verify required files exist."""
    config = Config()
    
    # Configure paths
    paths = {
        'data_root': config.DATA_ROOT,
        'raw_data': config.RAW_DATA_PATH,
        'processed_data': config.PROCESSED_DATA_PATH,
    }
    
    # Ensure processed data directory exists
    paths['processed_data'].mkdir(exist_ok=True)
    
    # Input files
    paths['atlas_data'] = paths['processed_data'] / "master_atlas_data_cleaned.csv"
    paths['boundaries'] = paths['raw_data'] / "atlas_admin2_boundaries.json"
    
    # SoilGrids raster files
    soil_dir = paths['raw_data'] / "soil" / "soilgrids"
    glosem_dir = paths['raw_data'] / "soil" / "GloSEM_25km" / "Data_25km"
    
    raster_files = {
        'soil_ph': soil_dir / "soilgrids_ph_0-5cm_ssa.tif",
        'soil_soc': soil_dir / "soilgrids_soc_0-30cm_ssa.tif", 
        'soil_sand': soil_dir / "soilgrids_sand_0-5cm_ssa.tif",
        'soil_clay': soil_dir / "soilgrids_clay_0-5cm_ssa.tif",
        'erosion_2012': glosem_dir / "RUSLE_SoilLoss_v1.1_yr2012_25km.tif"
    }
    
    return paths, raster_files

def load_base_data(paths):
    """Load Atlas master data and administrative boundaries."""
    print("ğŸ“Š Loading Atlas master data and administrative boundaries...")
    
    try:
        # Load cleaned Atlas data
        df = pd.read_csv(paths['atlas_data'])
        print(f"   âœ… Loaded Atlas data: {len(df):,} records")
        
        # Load administrative boundaries if available
        try:
            boundaries = gpd.read_file(paths['boundaries'])
            print(f"   âœ… Loaded admin boundaries: {len(boundaries):,} features")
            print(f"   ğŸ“‹ Boundary columns: {list(boundaries.columns)}")
            return df, boundaries
        except FileNotFoundError:
            print(f"   âš ï¸  Admin boundaries not found: {paths['boundaries']}")
            print(f"   ï¿½ Will use placeholder environmental data instead")
            return df, None
        
    except FileNotFoundError as e:
        print(f"âŒ ERROR: Could not find required Atlas data file.")
        print(f"   Missing file: {e.filename}")
        print(f"   Please ensure you've run the Atlas data pipeline first.")
        raise

def process_raster_data(df, boundaries, paths):
    """
    Process SoilGrids and GloSEM raster data using proper zonal statistics.
    
    Note: This processes actual environmental data with coordinate system handling.
    """
    print("ğŸŒ Processing environmental raster data...")
    
    # Check which raster files are available
    available_rasters = {}
    for key, filepath in paths['raster_files'].items():
        if filepath.exists():
            available_rasters[key] = filepath
            print(f"   âœ… Found: {key} - {filepath.name}")
        else:
            print(f"   âš ï¸  Missing: {key} - {filepath.name}")
    
    if not available_rasters:
        print("   âŒ No environmental raster files found!")
        print("   Please ensure SoilGrids and GloSEM data is downloaded")
        return add_placeholder_environmental_data(df)
    
    # Use zonal statistics if boundaries are available
    if boundaries is not None:
        return process_raster_with_boundaries(df, boundaries, available_rasters)
    else:
        print("   âš ï¸  No admin boundaries - using simulated environmental data")
        return add_placeholder_environmental_data(df)

def add_placeholder_environmental_data(df):
    """Add placeholder environmental data when raster processing isn't available."""
    print("   ğŸ“Š Adding placeholder environmental indicators...")
    
    # Add placeholder soil health indicators (would be replaced by actual zonal stats)
    np.random.seed(42)  # For reproducible placeholder data
    n_records = len(df)
    
    # Simulate soil health indicators
    df['soil_ph_mean'] = np.random.normal(6.5, 1.0, n_records).clip(4.0, 8.5)
    df['soil_soc_mean'] = np.random.exponential(2.0, n_records).clip(0.1, 10.0)
    df['soil_sand_mean'] = np.random.normal(45, 15, n_records).clip(10, 90)
    df['soil_clay_mean'] = np.random.normal(25, 10, n_records).clip(5, 60)
    df['erosion_2012_mean'] = np.random.exponential(5.0, n_records).clip(0.1, 50.0)
    
    print(f"   âœ… Added placeholder soil indicators to {len(df):,} records")
    return df

def process_raster_with_boundaries(df, boundaries, raster_files):
    """Process raster data using zonal statistics with admin boundaries."""
    import rasterio
    from rasterstats import zonal_stats
    
    print("   ğŸ—ºï¸  Processing raster data with zonal statistics...")
    
    # Ensure boundaries are in EPSG:4326 for consistency
    if boundaries.crs != 'EPSG:4326':
        print(f"   ğŸ”„ Reprojecting boundaries from {boundaries.crs} to EPSG:4326")
        boundaries = boundaries.to_crs('EPSG:4326')
    
    # Process each raster file
    for raster_key, raster_path in raster_files.items():
        print(f"   ğŸ“ˆ Processing {raster_key}...")
        
        try:
            # Check if raster needs reprojection
            with rasterio.open(raster_path) as src:
                raster_crs = src.crs.to_string()
                print(f"      Raster CRS: {raster_crs}")
                
                # Handle coordinate system differences
                if 'soilgrids' in str(raster_path) and raster_crs == 'ESRI:54009':
                    print(f"      âš ï¸  SoilGrids data in ESRI:54009 - may need reprojection")
                    # For now, we'll use boundaries reprojected to match raster
                    boundaries_for_zonal = boundaries.to_crs(raster_crs)
                else:
                    boundaries_for_zonal = boundaries
                
                # Calculate zonal statistics
                stats = zonal_stats(
                    boundaries_for_zonal.geometry,
                    raster_path,
                    stats=['mean', 'min', 'max', 'std'],
                    nodata=src.nodata
                )
                
                # Add statistics to boundaries
                for stat_name in ['mean', 'min', 'max', 'std']:
                    col_name = f"{raster_key}_{stat_name}"
                    boundaries[col_name] = [s[stat_name] if s[stat_name] is not None else np.nan for s in stats]
                
                print(f"      âœ… Added {raster_key} statistics")
                
        except Exception as e:
            print(f"      âŒ Error processing {raster_key}: {str(e)}")
            continue
    
    # Merge with atlas data based on admin columns
    print("   ğŸ”— Merging environmental data with Atlas data...")
    
    # Map admin boundary columns to atlas columns
    boundaries['country'] = boundaries['admin0_name']
    boundaries['region'] = boundaries['admin1_name'] 
    boundaries['sub_region'] = boundaries['admin2_name']
    
    # Merge on admin identifiers
    merge_cols = ['country', 'region', 'sub_region']
    env_cols = [col for col in boundaries.columns if any(
        raster_key in col for raster_key in raster_files.keys()
    )]
    
    df_merged = df.merge(
        boundaries[merge_cols + env_cols],
        on=merge_cols,
        how='left'
    )
    
    print(f"   âœ… Merged environmental data: {len(df_merged):,} records")
    return df_merged

def calculate_environmental_vulnerability(df):
    """Calculate environmental vulnerability scores from soil health indicators."""
    print("ğŸ§® Calculating environmental vulnerability scores...")
    
    # Environmental vulnerability components (higher = more vulnerable)
    # Low pH is worse (acidic soils)
    df['ph_vulnerability'] = 1 - ((df['soil_ph_mean'] - 4.0) / (8.5 - 4.0))
    df['ph_vulnerability'] = df['ph_vulnerability'].clip(0, 1)
    
    # Low SOC is worse (low fertility)
    df['soc_vulnerability'] = 1 - (df['soil_soc_mean'] / df['soil_soc_mean'].max())
    
    # High sand content is worse (drought prone)
    df['texture_vulnerability'] = df['soil_sand_mean'] / 100.0
    
    # Base vulnerability components
    env_components = ['ph_vulnerability', 'soc_vulnerability', 'texture_vulnerability']
    
    # Add erosion data if available (GloSEM has this)
    if 'erosion_2012_mean' in df.columns:
        df['erosion_vulnerability'] = df['erosion_2012_mean'] / df['erosion_2012_mean'].max()
        env_components.append('erosion_vulnerability')
        print("   âœ… Erosion data included in vulnerability assessment")
    else:
        print("   âš ï¸  Erosion data not available - using only pH, SOC, and texture")
    
    # Combined environmental vulnerability (mean of components)
    df['environmental_vulnerability_score'] = df[env_components].mean(axis=1)
    
    print(f"   âœ… Environmental vulnerability range: {df['environmental_vulnerability_score'].min():.3f} - {df['environmental_vulnerability_score'].max():.3f}")
    return df

def calculate_compound_risk(df):
    """Calculate final compound risk scores combining all vulnerability factors."""
    print("âš¡ Calculating compound risk scores...")
    
    # Combined vulnerability (social + environmental)
    df['combined_vulnerability_score'] = (
        df['social_vulnerability_score'] + 
        df['environmental_vulnerability_score']
    ) / 2
    
    # Final compound risk: Risk = Hazard Ã— Combined_Vulnerability
    df['compound_risk_score'] = (
        df['hazard_score'] * df['combined_vulnerability_score']
    )
    
    # Normalize to 0-1 scale
    df['compound_risk_score'] = df['compound_risk_score'] / df['compound_risk_score'].max()
    
    print(f"   âœ… Compound risk range: {df['compound_risk_score'].min():.3f} - {df['compound_risk_score'].max():.3f}")
    return df

def identify_hotspots(df, top_n=20):
    """Identify and analyze top risk hotspots."""
    print(f"ğŸ”¥ Identifying top {top_n} risk hotspots...")
    
    # Sort by compound risk score
    hotspots = df.nlargest(top_n, 'compound_risk_score')
    
    print("\n" + "="*80)
    print("ğŸ¯ TOP RISK HOTSPOTS")
    print("="*80)
    
    hotspot_summary = hotspots[[
        'country', 'region', 'sub_region', 
        'compound_risk_score', 'hazard_score', 'combined_vulnerability_score',
        'population', 'vop_crops_usd'
    ]].round(3)
    
    print(hotspot_summary.to_string(index=False))
    print("="*80)
    
    return hotspots

def export_results(df, hotspots, paths):
    """Export processed datasets for visualization and further analysis."""
    print("ğŸ’¾ Exporting processed datasets...")
    
    # Main dataset for visualization
    viz_columns = [
        'country', 'region', 'sub_region',
        'hazard_score', 'social_vulnerability_score', 'environmental_vulnerability_score',
        'combined_vulnerability_score', 'compound_risk_score',
        'population', 'vop_crops_usd', 'ndws_future_days', 'poverty_headcount_ratio',
        'soil_ph_mean', 'soil_soc_mean', 'soil_sand_mean', 'soil_clay_mean'
    ]
    
    # Export main dataset
    output_file = paths['processed_data'] / "compound_risk_assessment.csv"
    df[viz_columns].to_csv(output_file, index=False)
    print(f"   âœ… Main dataset: {output_file}")
    
    # Export hotspots
    hotspots_file = paths['processed_data'] / "risk_hotspots_top20.csv"
    hotspots[viz_columns].to_csv(hotspots_file, index=False)
    print(f"   âœ… Risk hotspots: {hotspots_file}")
    
    # Export summary statistics
    summary_stats = {
        'total_records': len(df),
        'countries_covered': df['country'].nunique(),
        'regions_covered': df['region'].nunique(),
        'subregions_covered': df['sub_region'].nunique(),
        'mean_compound_risk': float(df['compound_risk_score'].mean()),
        'max_compound_risk': float(df['compound_risk_score'].max()),
        'high_risk_threshold': 0.7,
        'high_risk_areas': int((df['compound_risk_score'] > 0.7).sum()),
        'total_population_at_risk': int(df[df['compound_risk_score'] > 0.7]['population'].sum()),
        'total_agricultural_value_at_risk': float(df[df['compound_risk_score'] > 0.7]['vop_crops_usd'].sum())
    }
    
    summary_file = paths['processed_data'] / "risk_assessment_summary.json"
    import json
    with open(summary_file, 'w') as f:
        json.dump(summary_stats, f, indent=2)
    print(f"   âœ… Summary statistics: {summary_file}")
    
    return summary_stats

def main():
    """Main execution function for geospatial processing and risk calculation."""
    print("ğŸŒ STEP 2: GEOSPATIAL PROCESSING & COMPOUND RISK CALCULATION")
    print("="*70)
    print("ğŸ“‹ Integrating Atlas socio-economic data with SoilGrids environmental data")
    print("ğŸ¯ Goal: Generate compound risk assessment for Sub-Saharan Africa")
    print()
    
    try:
        # Setup paths and verify files
        paths, raster_files = setup_paths()
        
        # Load base Atlas data
        df, boundaries = load_base_data(paths)
        
        # Process environmental raster data
        df = process_raster_data(df, boundaries, {'raster_files': raster_files, **paths})
        
        # Calculate environmental vulnerability
        df = calculate_environmental_vulnerability(df)
        
        # Calculate compound risk scores
        df = calculate_compound_risk(df)
        
        # Identify hotspots
        hotspots = identify_hotspots(df)
        
        # Export results
        summary_stats = export_results(df, hotspots, paths)
        
        # Final summary
        print("\n" + "ğŸ‰ GEOSPATIAL PROCESSING COMPLETE!")
        print("="*70)
        print(f"ğŸ“Š Processed {summary_stats['total_records']:,} sub-regions")
        print(f"ğŸŒ Coverage: {summary_stats['countries_covered']} countries")
        print(f"âš¡ High-risk areas: {summary_stats['high_risk_areas']} (threshold > 0.7)")
        print(f"ğŸ‘¥ Population at risk: {summary_stats['total_population_at_risk']:,}")
        print(f"ğŸ’° Agricultural value at risk: ${summary_stats['total_agricultural_value_at_risk']:,.0f}")
        print("\nğŸš€ Ready for Observable Framework visualization!")
        print("="*70)
        
    except Exception as e:
        print(f"âŒ ERROR: {str(e)}")
        print("Please check the error message above and ensure all prerequisites are met.")
        return False
    
    return True

if __name__ == "__main__":
    # Import numpy here to avoid issues if not available
    import numpy as np
    main()